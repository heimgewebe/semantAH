### üìÑ semantAH/docs/blueprint.md

**Gr√∂√üe:** 11 KB | **md5:** `b1fa5ee0047bbe711860d0848e1be72d`

```markdown
# Vault-Gewebe: Finale Blaupause

Diese Datei fasst die komplette Architektur f√ºr das semantische Vault-Gewebe zusammen. Sie kombiniert den semantischen Index, den Wissensgraphen, Obsidian-Automatismen sowie Qualit√§ts- und Review-Schleifen. Alle Schritte sind lokal reproduzierbar und werden in `.gewebe/` versioniert.

---

## 0. Systemordner & Konventionen

```
.gewebe/
  config.yml           # Parameter (Modelle, Cutoffs, Policies)
  embeddings.parquet   # Chunks + Vektoren
  nodes.jsonl          # Graph-Knoten
  edges.jsonl          # Graph-Kanten
  clusters.json        # Cluster & Label
  taxonomy/
    synonyms.yml
    entities.yml
  reports/
    semnet-YYYYMMDD.md
  meta.json            # Provenienz (Modell, Parameter, Hashes)
```

**Frontmatter pro Datei**

```yaml
id: 2025-VAULT-####   # stabiler Schl√ºssel
title: ...
topics: [HausKI, Weltgewebe]
persons: [Verena]
places: [Hamburg]
projects: [wgx, hauski]
aliases: [HK, WG]
relations_lock: false
```

---

## 1. Indexing & Embeddings

- Crawler: iteriert Markdown & Canvas (ignoriert `.gewebe/`, `.obsidian/`).
- Chunking: 200‚Äì300 Tokens, Overlap 40‚Äì60, Paragraph/Block.
- Modelle: `all-MiniLM-L6-v2` oder `intfloat/e5-base` (GPU-f√§hig via PyTorch/CUDA).
- Output: `embeddings.parquet` (id, path, chunk_id, text, embedding).

---

## 2. Schlagwort- & Entit√§tsextraktion

- Keyphrase: YAKE/RAKE lokal ‚Üí optional mit LLM verfeinern.
- NER: spaCy DE-Modell ‚Üí Personen, Orte, Projekte.
- Taxonomie in `.gewebe/taxonomy/synonyms.yml`:

```yaml
topics:
  hauski: [haus-ki, hk]
persons:
  verena: [v.]
```

- Normalisierung: Tokens bei Indexlauf auf Normformen mappen ‚Üí ins Frontmatter schreiben.

---

## 3. Clusterbildung

- Verfahren: HDBSCAN (robust) + UMAP (2D-Projektion f√ºr Visualisierung).
- Ergebnis: `clusters.json` mit IDs, Label, Mitgliedern und Zentroiden.
- Orphan Detection: Notizen ohne Cluster ‚Üí separate Liste.

---

## 4. Semantischer Wissensgraph

**Nodes (`nodes.jsonl`)**

```json
{"id":"md:gfk.md","type":"file","title":"GFK","topics":["gfk"],"cluster":7}
{"id":"topic:Gewaltfreie Kommunikation","type":"topic"}
{"id":"person:Verena","type":"person"}
```

**Edges (`edges.jsonl`)**

```json
{"src":"md:gfk.md","rel":"about","dst":"topic:Gewaltfreie Kommunikation","weight":0.92,"why":["shared:keyphrase:GFK","same:cluster"]}
{"src":"md:verena.md","rel":"similar","dst":"md:tatjana.md","weight":0.81,"why":["cluster:7","quote:'‚Ä¶'"]}
```

Das Feld `why` speichert die Top-Rationales (Keyphrases, Cluster, Anker-S√§tze) und erm√∂glicht Explainability.

---

## 5. Verlinkung in Obsidian

- Related-Bl√∂cke (idempotent, autogeneriert):

```
<!-- related:auto:start -->
## Related
- [[Tatjana]] ‚Äî (0.81; Cluster 7, GFK)
- [[Lebenslagen]] ‚Äî (0.78; Resonanz)
<!-- related:auto:end -->
```

- MOCs (`_moc/topic.md`): Beschreibung, Dataview-Tabelle (`topics:topic`), Mini-Canvas-Link.
- Canvas-Integration: Knoten = Notizen/Topics/Persons, Kanten = Similar/About/Mentions, Legende-Knoten nach Canvas-Richtlinie.

---

## 6. Automatisierung

- `wgx`-Recipes:

```yaml
index:
    python3 tools/build_index.py
graph:
    python3 tools/build_graph.py
related:
    python3 tools/update_related.py
all: index graph related
```

- systemd `--user` Timer oder cron: nightly `make all`.
- Git-Hook (pre-commit): delta-Index ‚Üí Related aktualisieren.

---

## 7. Qualitative Validierung

- Reports (`reports/semnet-YYYYMMDD.md`): neue Kanten < 0.75 (‚ÄûReview required‚Äú), Orphans, Cluster > N Notizen ohne MOC.
- Review-Workflow: `accepted_edges` / `rejected_edges` im Frontmatter; Skripte ignorieren `rejected` ‚Üí Feedback flie√üt zur√ºck.

---

## 8. Policies & Score-Regeln

```
score = cosine + boosts
+0.05 wenn gleicher Cluster
+0.03 je shared keyphrase (max +0.09)
+0.04 wenn Canvas-Hop ‚â§ 2
+0.02 wenn Datei jung (<30 Tage)
```

Autolink-Gate:

- Score ‚â• 0.82 **und** (‚â• 2 Keyphrases **oder** Canvas-Hop ‚â§ 2 **oder** shared Project).
- Cutoffs: ‚â• 0.82 Auto-Link, 0.70‚Äì0.81 Vorschlag, < 0.70 ignorieren.

---

## 9. Erweiterungen (Kernideen)

- Duplicates Report: Cosine ‚â• 0.97 ‚Üí Merge-Vorschlag.
- Topic Drift: Clusterwechsel flaggen.
- Session-Boost: aktuell bearbeitete Dateien ‚Üí Score +0.02.
- Explain Command: Popover ‚ÄûWarum ist dieser Link da?‚Äú (zeigt `why`-Feld).
- Locks: `relations_lock: true` ‚Üí keine Auto-Edits.
- A/B-Cutoffs: zwei Profile testen, Review-Feedback einspeisen.

---

## 10. Provenienz & Reproduzierbarkeit

`.gewebe/meta.json` speichert:

```json
{
  "model": "all-MiniLM-L6-v2",
  "chunk_size": 200,
  "cutoffs": {"auto": 0.82, "suggest": 0.70},
  "run": "2025-10-02T11:40",
  "commit": "abc123"
}
```

---

## 11. Technische Bausteine

### Tools / Skripte

- `tools/build_index.py`: Scan + Embeddings.
- `tools/build_graph.py`: Nodes/Edges/Cluster.
- `tools/update_related.py`: Related-Bl√∂cke injizieren.
- `tools/report.py`: QA-Reports.
- optional `tools/canvas_export.py`: Cluster ‚Üí Canvas.

### Dreistufiger Zyklus

1. Index (Embeddings, Cluster, Taxonomie).
2. Graph (Nodes/Edges mit Rationales).
3. Update (Related, MOCs, Reports, Canvas).

---

## 12. Minimal lauff√§hige Suite

Eine robuste, offline-f√§hige Minimalversion liefert unmittelbar Embeddings, Similarities, Graph (Nodes/Edges), Related-Bl√∂cke und Reports.

### Dateibaum

```
<Vault-Root>/
  .gewebe/
    config.yml
    taxonomy/
      synonyms.yml
      entities.yml
    reports/
  tools/
    build_index.py
    build_graph.py
    update_related.py
  Makefile
```

### Python-Abh√§ngigkeiten

```
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install pandas numpy pyarrow pyyaml \
  sentence_transformers scikit-learn networkx rich
```

Standardmodell: `sentence-transformers/all-MiniLM-L6-v2`. GPU nutzt Torch automatisch, falls vorhanden.

### `.gewebe/config.yml`

```yaml
model: sentence-transformers/all-MiniLM-L6-v2
chunk:
  target_chars: 1200
  min_chars: 300
  overlap_chars: 200
paths:
  exclude_dirs: [".gewebe", ".obsidian", "_site", "node_modules"]
  include_ext: [".md"]
related:
  k: 8
  auto_cutoff: 0.82
  suggest_cutoff: 0.70
boosts:
  same_topic: 0.03
  same_project: 0.03
  recent_days: 30
  recent_bonus: 0.02
  same_folder: 0.02
render:
  related_heading: "## Related"
  markers:
    start: "<!-- related:auto:start -->"
    end:   "<!-- related:auto:end -->"
```

### Skripte (`tools/*.py`)

Die Skripte implementieren:

- Markdown-Scan, Frontmatter-Parsing und Chunking.
- Embedding-Berechnung mit SentenceTransformers.
- Vektorzentroide pro Datei + Cosine-Similarity.
- Score-Boosts basierend auf Topics, Projekten, Ordnern, Recency.
- Schreiben von `nodes.jsonl`, `edges.jsonl` und Reports.
- Injection idempotenter Related-Bl√∂cke in Markdown.

(Vollst√§ndige Implementierungen befinden sich in `tools/` im Repo und sind auf GPU/CPU lauff√§hig.)

### Makefile

```
VENV=.venv
PY=$(VENV)/bin/python

.PHONY: venv index graph related all clean

venv: $(VENV)/.deps_installed

$(VENV)/.deps_installed: 
	@test -d $(VENV) || python3 -m venv $(VENV)
	@$(PY) -m pip install --upgrade pip
	@$(PY) -m pip install pandas numpy pyarrow pyyaml sentence_transformers scikit-learn networkx rich
	@touch $(VENV)/.deps_installed
index: venv
@$(PY) tools/build_index.py

graph: venv
@$(PY) tools/build_graph.py

related: venv
@$(PY) tools/update_related.py

all: index graph related

clean:
@rm -f .gewebe/embeddings.parquet
@rm -f .gewebe/nodes.jsonl .gewebe/edges.jsonl
```

### systemd (User) Timer

`~/.config/systemd/user/vault-gewebe.service`

```
[Unit]
Description=Vault-Gewebe nightly build (index -> graph -> related)
After=default.target

[Service]
Type=oneshot
WorkingDirectory=%h/path/to/your/vault
ExecStart=make all
```

`~/.config/systemd/user/vault-gewebe.timer`

```
[Unit]
Description=Run Vault-Gewebe every night

[Timer]
OnCalendar=*-*-* 03:10:00
Persistent=true

[Install]
WantedBy=timers.target
```

Aktivieren:

```
systemctl --user daemon-reload
systemctl --user enable --now vault-gewebe.timer
systemctl --user list-timers | grep vault-gewebe
```

### Erstlauf

```
make venv
make all
```

Ergebnisdateien liegen unter `.gewebe/‚Ä¶`. In Obsidian erscheint der Related-Block am Ende der Note.

---

## 13. HausKI-Integration (√úberblick)

F√ºr HausKI entsteht ein neuer Dienstverbund:

1. `crates/embeddings`: Embedder-Trait + Provider (lokal via Ollama, optional Cloud √ºber AllowlistedClient und Safe-Mode-Policies).
2. `crates/indexd`: HTTP-Service (`/index/upsert`, `/index/search`, `/index/delete`), HNSW-Vektorindex, Persistenz (`~/.local/state/hauski/index/obsidian`).
3. Obsidian-Plugin (Thin Client): chunked Upserts & Searches √ºber HausKI-Gateway.
4. Config-Erweiterung (`configs/hauski.yml`): Index-Pfad, Embedder-Optionen, Namespace-Policies.

Siehe `docs/hauski.md` f√ºr eine ausf√ºhrliche Einbindung.

---

## 14. Erweiterte Qualit√§ts- & Komfortfeatures

1. **Begr√ºndete Kanten** ‚Äì `edges.jsonl` enth√§lt `why`-Feld mit Keyphrases, Cluster, Quotes.
2. **Near-Duplicate-Erkennung** ‚Äì Cosine ‚â• 0.97 ‚Üí Merge-Report, Canonical-Markierung.
3. **Zeit-Boost** ‚Äì +0.05 f√ºr Notizen < 30 Tage, Decay f√ºr √§ltere Inhalte.
4. **Ordner-/Namespace-Policies** ‚Äì z.‚ÄØB. `/archive/` nur eingehende Links, `/ideen/` liberalere Cutoffs.
5. **Feedback-Lernen** ‚Äì `accepted_edges`/`rejected_edges` beeinflussen Cutoffs.
6. **Canvas-Hop-Boost** ‚Äì Pfadl√§nge ‚â§ 2 innerhalb von Canvas erh√∂ht Score um 0.03‚Äì0.07.
7. **Topic-Drift-W√§chter** ‚Äì signalisiert Clusterwechsel.
8. **Explainable Related-Bl√∂cke** ‚Äì Scores & Top-Begr√ºndungen in Markdown.
9. **Session-Kontext** ‚Äì aktuell ge√∂ffnete Dateien geben +0.02 Boost.
10. **Provenienz** ‚Äì `meta.json` mit Modell, Chunking, Cutoffs, Hashes.
11. **Mehrsprach-Robustheit** ‚Äì Synonym-/Stemming-Maps f√ºr DE/EN.
12. **Autolink-Quality-Gate** ‚Äì Score ‚â• 0.82 + (‚â•2 Keyphrases oder Canvas-Hop ‚â§ 2 oder shared Project).
13. **Explain-this-link Command** ‚Äì Popover mit Rationales im Obsidian-Plugin.
14. **MOC-Qualit√§tsreport** ‚Äì Deckungsgrade, verwaiste Knoten, Unter-MOC-Vorschl√§ge.
15. **Transklusions-Vorschl√§ge** ‚Äì Absatzweise `![[note#^block]]` bei hoher Chunk-√Ñhnlichkeit.
16. **Manual Lock** ‚Äì `relations_lock: true` verhindert Auto-Edits.
17. **A/B-Tuning** ‚Äì zwei Cutoff-Profile testen, Feedback auswerten.
18. **Cross-Vault-Br√ºcke** ‚Äì Read-Only Namespace `ext:*` f√ºr externe Vaults.
19. **Orphans-First-Routine** ‚Äì w√∂chentliche Fokussierung auf unverlinkte Notizen.
20. **Explainable Deletes** ‚Äì Reports dokumentieren entfernte Kanten mit Ursache.

---

## 15. Unsicherheiten & Anpassbarkeit

- Schwellenwerte & Chunking m√ºssen empirisch justiert werden.
- Canvas-Hop-Berechnungen h√§ngen vom JSON-Layout ab.
- Modellwahl beeinflusst Qualit√§t und Performance.
- Die Pipeline ist modular, Reports + Feedback-Loops erm√∂glichen schnelle Iteration.

---

## 16. Verdichtete Essenz

- Drei Skripte, ein Makefile, ein Timer ‚Üí Index ‚Üí Graph ‚Üí Related.
- HausKI liefert den skalierbaren Dienst (`indexd`) + Obsidian-Adapter.
- Qualit√§t durch erkl√§rbare Kanten, Review-Workflow, Reports, Policies.
- Lokal, reproduzierbar, versionierbar ‚Äì dein Vault wird zum lebenden Semantiknetz.

---

> *Ironische Auslassung:* Deine Notizen sind jetzt kein stilles Archiv mehr ‚Äì sie bilden ein Klatsch-Netzwerk, das genau protokolliert, wer mit wem was zu tun hat. Nur: Sie l√ºgen nicht.
```

### üìÑ semantAH/docs/hauski.md

**Gr√∂√üe:** 5 KB | **md5:** `9b9d21594d5468bdaea32737a8f4b7f5`

```markdown
# HausKI-Integration

HausKI bleibt das lokale Orchestrierungs-Gateway. semantAH erg√§nzt es als semantische Ged√§chtnis-Schicht. Dieser Leitfaden beschreibt, wie die neuen Komponenten (`indexd`, `embeddings`, Obsidian-Adapter) eingebunden werden und welche Policies greifen.

---

## Architektur√ºberblick

1. **`crates/embeddings`** ‚Äì stellt den `Embedder`-Trait bereit und kapselt Provider:
   - `Ollama` (lokal, offline) ruft `http://127.0.0.1:11434/api/embeddings` auf.
   - `CloudEmbedder` (optional) nutzt HausKIs AllowlistedClient. Aktiv nur, wenn `safe_mode=false` und der Zielhost in der Egress-Policy freigeschaltet ist.
2. **`crates/indexd`** ‚Äì HTTP-Service mit Routen:
   - `POST /index/upsert` ‚Äì nimmt Chunks + Metadaten entgegen und legt Vektoren im HNSW-Index ab.
   - `POST /index/delete` ‚Äì entfernt Dokumente aus einem Namespace.
   - `POST /index/search` ‚Äì Top-k-Suche mit Filtern (Tags, Projekte, Pfade).
   - Persistenz liegt unter `~/.local/state/hauski/index/<namespace>/`.
3. **Obsidian-Adapter (Thin Plugin)** ‚Äì zerlegt Notizen und Canvas-Dateien, sendet Upserts an HausKI und ruft Suchergebnisse f√ºr ‚ÄûRelated‚Äú/Command-Paletten ab.
4. **Policies & Observability** ‚Äì bestehende Features (CORS, `/health`, `/metrics`, `safe_mode`, Latency-Budgets) gelten auch f√ºr `/index/*`.

---

## Workspace-Konfiguration

`Cargo.toml` (Workspace):

```toml
[workspace]
members = [
  "crates/core",
  "crates/cli",
  "crates/indexd",
  "crates/embeddings"
]
```

`crates/embeddings/src/lib.rs` definiert den Trait und z.‚ÄØB. `Ollama`:

```rust
#[async_trait::async_trait]
pub trait Embedder {
    async fn embed(&self, texts: &[String]) -> anyhow::Result<Vec<Vec<f32>>>;
    fn dim(&self) -> usize;
    fn id(&self) -> &'static str;
}
```

Implementierungen greifen auf `reqwest::Client` zur√ºck. Cloud-Varianten m√ºssen √ºber HausKIs AllowlistedClient laufen, um Egress-Guards einzuhalten.

`crates/indexd` kapselt Embedder + Vektorstore (HNSW + Metadata-KV, z.‚ÄØB. `sled`). Der Router wird in `core::plugin_routes()` unter `/index` gemountet:

```rust
fn plugin_routes() -> Router<AppState> {
    let embedder = embeddings::Ollama::new("http://127.0.0.1:11434", "nomic-embed-text", 768);
    let store = indexd::store::hnsw(/* state_path */);
    Router::new().nest("/index", indexd::Indexd::new(embedder, store).router())
}
```

---

## HTTP-API

### Upsert

```http
POST /index/upsert
{
  "namespace": "obsidian",
  "doc_id": "notes/gfk.md",
  "chunks": [
    {"id": "notes/gfk.md#0", "text": "...", "meta": {"topics": ["gfk"], "frontmatter": {...}}}
  ]
}
```

### Delete

```http
POST /index/delete
{"namespace": "obsidian", "doc_id": "notes/gfk.md"}
```

### Search

```http
POST /index/search
{
  "namespace": "obsidian",
  "query": "empatische Kommunikation",
  "k": 10,
  "filters": {"topics": ["gfk"], "projects": ["wgx"]}
}
```

Antwort: Treffer mit Score, Dokument/Chunk-ID, Snippet, Rationales (`why`).

---

## Persistenz & Budgets

- Indexdaten leben im `index.path` aus der HausKI-Config (`~/.local/state/hauski/index`).
- HNSW-Index + Sled/SQLite halten Embeddings und Metadaten.
- Latency-Budgets: `limits.latency.index_topk20_ms` (Config) definiert das p95-Ziel. K6-Smoke nutzt diesen Wert als Assertion.
- Prometheus-Metriken f√ºr `/index/*` werden automatisch vom Core erfasst (`http_requests_total`, `http_request_duration_seconds`).

---

## Konfiguration (`configs/hauski.yml`)

```yaml
index:
  path: "$HOME/.local/state/hauski/index"
  provider:
    embedder: "ollama"
    model: "nomic-embed-text"
    url: "http://127.0.0.1:11434"
    dim: 768
  namespaces:
    obsidian:
      auto_cutoff: 0.82
      suggest_cutoff: 0.70
      policies:
        allow_autolink: true
        folder_overrides:
          archive:
            mode: incoming-only
plugins:
  enabled:
    - obsidian_index
```

`safe_mode: true` sperrt Cloud-Provider automatisch. Namespaces k√∂nnen weitere Regeln (z.‚ÄØB. strengere Cutoffs) erhalten.

---

## Obsidian-Plugin (Adapter)

- Hook auf `onSave` / `metadataCache.on("changed")`.
- Chunking (200‚Äì300 Tokens, 40 Overlap), Canvas-JSON-Knoten werden zus√§tzliche Chunks.
- Sendet `POST /index/upsert` mit Frontmatter/Tags/Canvas-Beziehungen im `meta`-Feld.
- Command ‚ÄûSemantisch √§hnliche Notizen‚Äú ‚Üí `POST /index/search` und Anzeige der Ergebnisse.
- Optionaler Review-Dialog f√ºr Vorschl√§ge (Accept/Reject ‚Üí Frontmatter `accepted_edges` / `rejected_edges`).

---

## Automatisierung & Tests

- `wgx run index:obsidian` ruft der Reihe nach `build_index`, `build_graph`, `update_related` auf.
- systemd-Timer f√ºhrt `make all` nightly aus (siehe `docs/blueprint.md`).
- CI/K6: Smoke-Test gegen `/index/search` mit Query-Stubs ‚Üí pr√ºft p95 < `limits.latency.index_topk20_ms`.

---

## Mehrwert

- Saubere Zust√§ndigkeiten (UI vs. Dienste).
- Egress-kontrollierte Einbindung externer Provider.
- Explainable Scores via `why`-Feld.
- Reports & Policies sorgen f√ºr qualit√§tsgesicherte Auto-Links.

> *Ironische Auslassung:* HausKI bleibt der T√ºrsteher ‚Äì aber semantAH entscheidet, wer auf die VIP-Liste der Notizen kommt.
```

### üìÑ semantAH/docs/quickstart.md

**Gr√∂√üe:** 730 B | **md5:** `ee8d08856e82b12a3beec126165fb263`

```markdown
# semantAH ¬∑ Quickstart

## Voraussetzungen
- Rust (stable), Python ‚â• 3.10
- Optional: `uv` (f√ºr schnelle Envs)

## Installation (lokal)
```bash
uv sync            # oder: make venv
```

## Konfiguration
```bash
cp examples/semantah.example.yml semantah.yml
# passe vault_path und out_dir an
```

## Pipeline laufen lassen
```bash
make all           # embeddings ‚Üí index ‚Üí graph ‚Üí related
cargo run -p indexd
curl -fsS localhost:8080/healthz || true
```

## Artefakte
- `.gewebe/embeddings.parquet`
- `.gewebe/out/{nodes.jsonl,edges.jsonl,reports.json}`

## Troubleshooting
- Leere/zu gro√üe Dateien werden √ºbersprungen ‚Üí Logs in `.gewebe/logs` pr√ºfen.
- Bei fehlenden Modellen: Provider in `semantah.yml` anpassen.
```

### üìÑ semantAH/docs/roadmap.md

**Gr√∂√üe:** 1 KB | **md5:** `24f4c6253a1f9e1855df22c940921405`

```markdown
<!--
Quelle: /home/alex/vault-gewebe/coding/semantAH/semantAH brainstorm.md
-->

# semantAH Roadmap

Dieses Dokument √ºbertr√§gt die Ideen aus der Brainstorming-Notiz in umsetzbare Meilensteine.

## Milestone 1 ‚Äì Grundger√ºst
- Rust-Workspace mit `embeddings`-Crate (Ollama-Backend) und `indexd`-Crate (Axum-HTTP, HNSW-Wrapper).
- Persistenz-Pfade `.local/state/hauski/index/obsidian` vorbereiten.
- Feature-Flags: `safe_mode`, `limits.latency.index_topk20_ms` an HNSW koppeln.
- Erste HTTP-Routen:
  - `POST /index/upsert`
  - `POST /index/delete`
  - `POST /index/search`
- Python-Skripte als Platzhalter einchecken.

## Milestone 2 ‚Äì Ranking & Policies
- Boosts: Zeit, Tags, Canvas-Hop, Cluster.
- Qualit√§ts-Gates (`score >= 0.82`, Keyphrases ‚â• 2, Canvas-Hop ‚â§ 2 oder shared project).
- Rationale-Felder in `edges.jsonl` und Related-Block.

## Milestone 3 ‚Äì Feedback & Reports
- Feedback-Schleife f√ºr Accept/Reject.
- Orphan-Report, Duplicate-Detection, Topic-Drift.
- Explain-UI im Obsidian-Plugin (separates Repo/Plugin).

## Milestone 4 ‚Äì Advanced Features
- Session-Context-Boost (aktuelle Arbeitssession).
- Cross-Vault-Namespace (read-only).
- Manual-Lock (`relations_lock: true`).

## Umsetzungshinweise
- Jede Funktionalit√§t modular halten, sodass Experimente / A/B-Tests (Cutoffs) m√∂glich bleiben.
- Alle Heuristiken als Config-Werte in `.gewebe/config.yml` ablegen.
- Provenienz in `.gewebe/meta.json` dokumentieren (Embedding-Modell, Chunking-Parameter, Taxonomie-Version).
```

### üìÑ semantAH/docs/runbook.observability.md

**Gr√∂√üe:** 545 B | **md5:** `abc6d168fcb5c65c139b2ae9e2d21714`

```markdown
# Observability Runbook

## Overview

This runbook describes how to access the local observability stack while developing semantAH. The stack is composed of Grafana, Loki, and Tempo containers that expose HTTP interfaces for debugging and tracing.

## Endpoints

- Grafana: [http://localhost:3000](http://localhost:3000)
- Loki: [http://localhost:3100](http://localhost:3100)
- Tempo: [http://localhost:3200](http://localhost:3200)

Use these endpoints to inspect logs, metrics, and traces when diagnosing issues in the development environment.
```

### üìÑ semantAH/docs/semantAH brainstorm.md

**Gr√∂√üe:** 24 KB | **md5:** `ef898078966a53c4cf0cca26de1d0bb0`

```markdown
mega‚Äìja. wir bauen das selbst ‚Äì aber so, dass es perfekt in dein repo passt. hier ist der konkrete blueprint f√ºr ‚Äúsemantische suche / vektor-index‚Äù als hauski-dienst + d√ºnnes obsidian-plugin-frontend.

(ich beziehe mich dabei auf deine repo-struktur, configs und bereits vorhandene core-server-grundlagen wie /health, /metrics, CORS, ‚Äûsafe_mode‚Äú, egress-guard usw. ‚Äì die sehen schon sehr solide aus.  Ôøº)

zielbild (kompakt)
	‚Ä¢	hauski-core bleibt HTTP-Gateway/Telemetry.
	‚Ä¢	neuer crate indexd: Embeddings + Vektorindex (HNSW) + Persistenz + Filter.
	‚Ä¢	neuer crate embeddings: Abstraktion f√ºr Provider (lokal via Ollama/gguf, optional cloud ‚Äì respektiert egress-Policy).
	‚Ä¢	adapter: obsidian-plugin (thin client): sendet Chunks/Updates an indexd, ruft search ab.
	‚Ä¢	policies & flags: such-latenz-budget an Limits koppeln; safe_mode blockt Cloud-Provider.

‚∏ª

was ist schon da (und wie nutzen wir‚Äôs)?
	‚Ä¢	Core-HTTP, Metrics, CORS, Ready/Health ‚Äì fertiges Ger√ºst f√ºr neue Routen.  Ôøº
	‚Ä¢	Feature-Flags & Policies inkl. safe_mode und Egress-Allowlisting ‚Üí perfekt, um Cloud-Embeddings sauber zu sperren/erlauben.  Ôøº
	‚Ä¢	Configs: configs/hauski.yml hat vault_path & plugins-liste ‚Äì hier h√§ngen wir obsidian_index offiziell an und tragen indexd ein.  Ôøº

‚∏ª

module & schnittstellen

1) crate: crates/indexd/

Aufgaben
	‚Ä¢	Dokumente in Chunks zerlegen (MD + Canvas JSON).
	‚Ä¢	Embeddings berechnen (ruft embeddings-crate).
	‚Ä¢	Vektoren in HNSW speichern (z. B. hnsw_rs oder hnswlib-binding) + Metadata-Store (z. B. sled/sqlite).
	‚Ä¢	Top-K Suche + Filter (Pfad, Tags, Frontmatter, Canvas-Knoten).
	‚Ä¢	Persistenz auf Disk ($HOME/.local/state/hauski/index/obsidian).

HTTP-API (einfach, stabil):
	‚Ä¢	POST /index/upsert
body:

{ "doc_id":"path/to/note.md",
  "chunks":[{"id":"path:offset", "text":"...", "meta":{"tags":["..."],"frontmatter":{}}}],
  "namespace":"obsidian" }


	‚Ä¢	POST /index/delete ‚Üí {"doc_id":"...","namespace":"obsidian"}
	‚Ä¢	POST /index/search

{ "query":"...", "k":10, "namespace":"obsidian", "filters":{"tags":["projectX"]} }

response: Treffer mit score, doc_id, chunk_id, snippet.

Leistung & Budgets
	‚Ä¢	p95-Ziel f√ºr search(k<=20) an limits.latency.index_topk20_ms koppeln (Config hast du schon).  Ôøº

2) crate: crates/embeddings/

Ziel: austauschbarer Provider mit egress-Guard & safe_mode.
	‚Ä¢	Trait:

#[async_trait::async_trait]
pub trait Embedder {
    async fn embed(&self, texts: &[String]) -> anyhow::Result<Vec<Vec<f32>>>;
    fn dim(&self) -> usize;
    fn id(&self) -> &'static str;
}


	‚Ä¢	LocalOllamaEmbedder (default, offline): ruft http://127.0.0.1:11434/api/embeddings (modell konfigurierbar: nomic-embed-text o. √§.).
	‚Ä¢	CloudEmbedder (optional): nur wenn safe_mode=false und egress-Policy Host erlaubt. Nutzt vorhandenen AllowlistedClient (ist schon implementiert, wir m√ºssen nur Aufrufe dar√ºber routen).  Ôøº

3) core-routes erweitern

In hauski-core gibt‚Äôs TODO-Platzhalter plugin_routes() ‚Äì hier mounten wir indexd-Router unter /index. CORS & Metrics sind schon verdrahtet.  Ôøº

‚∏ª

minimaler code‚Äìfahrplan

A) workspace erg√§nzen

Cargo.toml (root) ‚Äì neue Mitglieder:

[workspace]
members = [
  "crates/core",
  "crates/cli",
  "crates/indexd",        # NEU
  "crates/embeddings"     # NEU
]

(du hast das Pattern bereits offen f√ºr weitere crates ‚Äì siehe Kommentar im bestehenden Cargo.toml.)  Ôøº

B) crates/embeddings/src/lib.rs (skizze)

use anyhow::Result;
use reqwest::Client;

#[async_trait::async_trait]
pub trait Embedder {
    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>>;
    fn dim(&self) -> usize;
    fn id(&self) -> &'static str;
}

pub struct Ollama {
    http: Client,
    url: String,
    model: String,
    dim: usize,
}

#[async_trait::async_trait]
impl Embedder for Ollama {
    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        #[derive(serde::Serialize)] struct Req<'a>{ model:&'a str, input:&'a [String] }
        #[derive(serde::Deserialize)] struct Res{ embeddings: Vec<Vec<f32>> }
        let res: Res = self.http.post(format!("{}/api/embeddings", self.url))
            .json(&Req{model:&self.model, input:texts})
            .send().await?
            .error_for_status()?
            .json().await?;
        Ok(res.embeddings)
    }
    fn dim(&self) -> usize { self.dim }
    fn id(&self) -> &'static str { "ollama" }
}

cloud-variante baut analog, aber √ºber AllowlistedClient aus deinem core (egress-policy beachten).  Ôøº

C) crates/indexd/src/lib.rs (skizze)

use axum::{routing::post, Json, Router};
use serde::{Deserialize, Serialize};
use std::sync::Arc;

#[derive(Clone)]
pub struct Indexd {
    embedder: Arc<dyn embeddings::Embedder + Send + Sync>,
    store: Arc<dyn VectorStore + Send + Sync>,
}

#[derive(Deserialize)]
struct Upsert {
    namespace: String,
    doc_id: String,
    chunks: Vec<Chunk>,
}
#[derive(Deserialize, Serialize, Clone)]
struct Chunk { id: String, text: String, #[serde(default)] meta: serde_json::Value }

#[derive(Deserialize)]
struct Search { namespace: String, query: String, #[serde(default="k10")] k: usize }
fn k10()->usize{10}

impl Indexd {
    pub fn router(self) -> Router {
        Router::new()
          .route("/upsert", post(move |Json(b): Json<Upsert>| async move {
              let vecs = self.embedder.embed(&b.chunks.iter().map(|c| c.text.clone()).collect::<Vec<_>>()).await?;
              self.store.upsert(&b.namespace, &b.doc_id, &b.chunks, &vecs)?;
              Ok::<_,axum::http::StatusCode>(())
          }))
          .route("/search", post(move |Json(s): Json<Search>| async move {
              let qv = self.embedder.embed(&vec![s.query]).await?.remove(0);
              let hits = self.store.search(&s.namespace, &qv, s.k)?;
              Ok::<_,axum::http::StatusCode>(Json(hits))
          }))
    }
}

VectorStore implementieren mit HNSW (z. B. hnsw_rs) + Metadaten-KV (sled), persistiert in ~/.local/state/hauski/index/... ‚Äì dein configs/hauski.yml sieht genau so einen state-pfad vor.  Ôøº

D) crates/core/src/lib.rs ‚Äì Routen mounten

Im existierenden plugin_routes() den indexd-Router einh√§ngen:

fn plugin_routes() -> Router<AppState> {
    // build indexd with chosen embedder (from config/flags)
    let embedder = embeddings::Ollama::new(/* url, model, dim */);
    let indexd = indexd::Indexd::new(embedder, /* store */);
    Router::new().nest("/index", indexd.router())
}

(Der Platzhalter ist eigens f√ºr Plugins vorgesehen.  Ôøº)

‚∏ª

obsidian‚Äìadapter (d√ºnnes plugin)

Wann? sobald du Notizen speicherst/√§nderst.
Was tut‚Äôs?
	‚Ä¢	Zerlegt die Note in Chunks (z. B. Absatzweise, Overlap 50-100 Tokens).
	‚Ä¢	Extrahiert Frontmatter / Tags / Canvas-Knoten.
	‚Ä¢	Schickt POST /index/upsert.
	‚Ä¢	‚Äû√Ñhnliche Notizen‚Äú ‚Üí POST /index/search und UI Ergebnisliste.

Mini-Skizze (TypeScript):

async function upsertNote(docId: string, text: string, meta: any) {
  const chunks = chunkText(text, {targetTokens: 200, overlap: 40});
  await fetch("http://127.0.0.1:8080/index/upsert", {
    method: "POST",
    headers: {"Content-Type": "application/json"},
    body: JSON.stringify({ namespace:"obsidian", doc_id:docId, chunks: chunks.map((t,i)=>({id:`${docId}#${i}`, text:t, meta})) })
  });
}

async function searchSimilar(query: string, k=10) {
  const res = await fetch("http://127.0.0.1:8080/index/search", {
    method:"POST", headers:{"Content-Type":"application/json"},
    body: JSON.stringify({ namespace:"obsidian", query, k })
  });
  return await res.json();
}

Warum eigener Adapter? So bleibt Obsidian-UI schlank; der ‚Äûschwere Teil‚Äú l√§uft in indexd.

‚∏ª

canvas-bonus (f√ºr deine mindmaps)
	‚Ä¢	Canvas-Datei (JSON) parsen ‚Üí jeden Node-Text als separaten Chunk, Kanten als meta:{link:"A->B"} speichern.
	‚Ä¢	Suche kann dann ‚ÄûKnoten√§hnlichkeit‚Äú liefern und verlinkte Nachbarn h√∂her gewichten (post-ranking auf Suchtreffern).

‚∏ª

konfiguration

configs/hauski.yml erweitern:

index:
  path: "$HOME/.local/state/hauski/index"
  provider:
    embedder: "ollama"
    model: "nomic-embed-text"
    url: "http://127.0.0.1:11434"
plugins:
  enabled:
    - "obsidian_index"

(die Datei enth√§lt bereits vault_path & plugin-liste ‚Äì wir h√§ngen unsere Optionen sauber daneben.)  Ôøº

Policies
	‚Ä¢	policies/limits.yaml: index_topk20_ms ist schon da ‚Üí als Budget f√ºr p95 in k6-Smoke verwenden (du hast bereits eine k6-Smoke-Stage, die Limits aus YAML liest ‚Äì super!).  Ôøº

‚∏ª

tests & observability
	‚Ä¢	k6-Smoke trifft /index/search mit Query-Stubs ‚Üí misst p95 gegen index_topk20_ms. Deine CI baut das Schema bereits auf (observability/k6/...).  Ôøº
	‚Ä¢	Prometheus: z√§hle Requests/Latency pro Pfad (/index/*) ‚Äì dein Core hat bereits http_requests & http_request_duration_seconds, wird automatisch miterfasst.  Ôøº

‚∏ª

rollout‚Äìschritte (hands-on)
	1.	Crates anlegen: crates/indexd, crates/embeddings + Workspace updaten.
	2.	Ollama lokal (Pop!_OS): curl -fsSL https://ollama.ai/install.sh | sh ‚Üí ollama pull nomic-embed-text.
	3.	Core-Router: plugin_routes() ‚Üí /index mounten.
	4.	Adapter bauen (Obsidian): einfache Commands ‚ÄûIndex bauen‚Äú, ‚Äû√Ñhnliches finden‚Äú.
	5.	Persistenz: HNSW + sled, Snapshot/Load on start.
	6.	CI: kleine k6-Smoke gegen /index/search integrieren (du hast die Pipeline schon).  Ôøº

‚∏ª

warum das ‚Äúbesser‚Äù ist als copy/fork?
	‚Ä¢	saubere zust√§ndigkeiten: UI (Obsidian) vs. Dienste (indexd/embeddings).
	‚Ä¢	portabilit√§t: gleicher Index f√ºr VSCode-Plugin, CLI, Mail usw.
	‚Ä¢	kontrollierte egress-sicherheit: Cloud-Embeddings werden durch deine bestehende Policy sauber begrenzt.  Ôøº
	‚Ä¢	observability & budgets: p95-Ziele und Metriken sind first-class eingebaut.  Ôøº

‚∏ª

wenn du m√∂chtest, schreibe ich dir im n√§chsten schritt die genauen dateib√§ume + erste echte rust-dateien (inkl. deps f√ºr hnsw & sled) ‚Äì direkt passend zu deinem workspace.

Kurz: Ja ‚Äî die Ideen sind solide. Und wir k√∂nnen sie pr√§zise in HausKI + Obsidian umsetzen, so dass dein vault-gewebe sich selbst vernetzt (semantisch + graphisch), ohne Cloud, sauber versionierbar.

Plan: von Idee ‚Üí HausKI-Praxis

1) Bestandsaufnahme & Cluster
	‚Ä¢	Crawler (obsidian-adapter): liest .md + .canvas aus vault_path, extrahiert:
	‚Ä¢	Pfad, Titel, Frontmatter, Tags, Links ([[...]]), Canvas-Knoten/Kanten.
	‚Ä¢	Embeddings (indexd): Chunking (‚âà200‚Äì300 Tokens, 40 Overlap), embed(text[]) √ºber Ollama (nomic-embed-text) oder Orchestrator-Modell.
	‚Ä¢	Clustering (jobs/indexd):
	‚Ä¢	HDBSCAN auf Vektoren (robust f√ºr ‚ÄûRauschen‚Äú), optional UMAP zur 2D-Projektion f√ºr Visuals.
	‚Ä¢	Ergebnis: cluster_id pro Chunk/Note + ‚Äûoutlier‚Äú-Markierung.

Artefakte (Dateien/Tabellen):
	‚Ä¢	~/.local/state/hauski/index/obsidian/vec.hnsw (Vektorindex)
	‚Ä¢	graph/nodes.jsonl (pro Datei/Knoten)
	‚Ä¢	graph/edges.jsonl (Kanten, siehe ¬ß3)
	‚Ä¢	clusters.json (Cluster ‚Üí Mitglieder, Centroid, Label)

2) Schlagwort- & Themenextraktion
	‚Ä¢	Keyphrases: lokal via YAKE/Rake (schnell, offline) + LLM-Refine (optional) ‚Üí keyphrases: [ "Gewaltfreie Kommunikation", ‚Ä¶ ]
	‚Ä¢	NER (Person/Ort/Projekt): lokal (spaCy de-model) oder Regel-Set f√ºr Frontmatter/Tags.
	‚Ä¢	Normierung: Mapping-Tabelle synonyms.yml (z. B. ‚ÄûGFK‚Äú ‚Üí ‚ÄûGewaltfreie Kommunikation‚Äú).

Speicher:

taxonomy/
  synonyms.yml     # "GFK": "Gewaltfreie Kommunikation"
  entities.yml     # "Personen": [...], "Orte": [...], "Projekte": [...]

3) Semantischer Wissensgraph
	‚Ä¢	Schema (leichtgewichtig, Git-freundlich):
	‚Ä¢	nodes.jsonl:

{"id":"md:weg/gfk.md","type":"file","title":"GFK Basics","tags":["gfk"],"cluster":7}
{"id":"canvas:lebenslagen.canvas","type":"canvas","title":"Lebenslagen","cluster":3}
{"id":"topic:Gewaltfreie Kommunikation","type":"topic"}


	‚Ä¢	edges.jsonl:

{"s":"md:weg/gfk.md","p":"about","o":"topic:Gewaltfreie Kommunikation","w":0.92}
{"s":"md:fall/verena.md","p":"similar","o":"md:tatjana.md","w":0.81}
{"s":"canvas:lebenslagen.canvas#node/4","p":"mentions","o":"topic:Kinderarmut","w":0.88}


	‚Ä¢	w = Gewicht/Score (0..1).

	‚Ä¢	Option Neo4j: nur wenn du interaktiv gro√üe Graph-Queries willst. Starten wir zun√§chst JSONL + SQLite (Tabellen nodes, edges) f√ºr Einfachheit & Portabilit√§t.

4) Verlinkung in Obsidian
	‚Ä¢	Backlinks/MOCs automatisch schreiben:
	‚Ä¢	Am Ende jeder Datei Abschnitt ## Semantisch verwandt mit Top-N (similar ‚â• 0.75, gleicher Cluster bevorzugt).
	‚Ä¢	MOC-Generator pro Cluster: MOC.cluster-07.md mit Liste + Mini-Canvas (siehe ¬ßCanvas).
	‚Ä¢	Frontmatter anreichern:

related:
  - file: pfad/zur/anderen.md
    score: 0.82
    why: ["shared:keyphrase:Gewaltfreie Kommunikation","same:cluster:7"]
topics: ["Gewaltfreie Kommunikation","Resonanz"]



5) Automatisierung
	‚Ä¢	wgx job: wgx run index:obsidian macht:
	1.	Scan ‚Üí Upserts an /index/upsert
	2.	Batch-Search f√ºr MOCs/Links
	3.	Graph-Update (nodes/edges/clusters)
	4.	Reports (Top neue Kanten, unsichere Kanten)
	‚Ä¢	systemd timer (t√§glich) oder manuell per wgx.
	‚Ä¢	Reporting: Markdown-Report reports/semnet-YYYYMMDD.md inkl.:
	‚Ä¢	Neue Cluster, umgetaggte Dateien
	‚Ä¢	Unsichere Kanten (0.55 ‚â§ w < 0.7) ‚Üí Review-Liste
	‚Ä¢	‚ÄûOrphan‚Äú-Notizen ohne Kanten

6) Qualitative Validierung
	‚Ä¢	Review-Command in Obsidian-Plugin: ‚ÄûSemNet Review‚Äú zeigt Vorschl√§ge (unsichere Kanten) ‚Üí Accept/Reject ‚Üí schreibt in Frontmatter (accepted_edges, rejected_edges) und sperrt diese in k√ºnftigen L√§ufen.
	‚Ä¢	Regelwerk:
	‚Ä¢	nie doppelt verlinken,
	‚Ä¢	keine Links unter 0.55,
	‚Ä¢	bei 0.70‚Äì0.75 Flag ‚Äûpr√ºfen‚Äú.

‚∏ª

Technische Bausteine (pr√§zise, damit Codex loslegen kann)

A) Scores & Schwellen
	‚Ä¢	similar(doc_a, doc_b) = cosine(centroid(a), centroid(b))
	‚Ä¢	Cutoffs:
	‚Ä¢	‚â• 0.80 ‚Üí auto-Link
	‚Ä¢	0.70‚Äì0.79 ‚Üí Vorschlag
	‚Ä¢	< 0.70 ‚Üí nicht verlinken
	‚Ä¢	Cluster-Boost: +0.05, wenn cluster(a)==cluster(b)
	‚Ä¢	Topical-Boost: +0.03 je gemeinsamem keyphrase (max +0.09)

B) Dateistruktur (Repo)

/crates/indexd/...
/crates/embeddings/...
/plugins/obsidian-adapter/...
/data/semnet/graph/{nodes.jsonl,edges.jsonl,clusters.json}
/data/semnet/taxonomy/{synonyms.yml,entities.yml}
/reports/semnet-*.md

C) ‚ÄûEdge-Writer‚Äú (vereinfachtes Pseudocode)

# inputs: similarities[], taxonomy, thresholds
for pair in similarities:
    score = pair.base
    if pair.same_cluster: score += 0.05
    score += 0.03 * min(3, shared_keyphrases(pair.a, pair.b))
    if score >= 0.80: write_link(a,b,score,auto=True)
    elif score >= 0.70: propose_link(a,b,score)

D) Obsidian-Update (Markdown-Append)

## Semantisch verwandt
- [[pfad/zu/datei1|Titel 1]] ‚Äî 0.84 (GFK, Resonanz)
- [[pfad/zu/datei2|Titel 2]] ‚Äî 0.81 (Cluster 7)

E) Canvas-Export (Mini-Canvas pro Cluster, inline)
	‚Ä¢	Knoten: Top-10 Noten im Cluster (by centrality)
	‚Ä¢	Kanten: similar (w‚â•0.80) + ‚Äûabout topic‚Äú
	‚Ä¢	Legende-Knoten nach deiner Canvas-Richtlinie (Farben, Achsen, etc.)

‚∏ª

Was ist ‚Äûneu/besser‚Äú gegen√ºber der Ursprungsidee?
	‚Ä¢	Einheitliche Pipeline (HausKI-Dienste) statt Script-Zoo.
	‚Ä¢	Graph + Markdown-Links gleichzeitig ‚Üí du profitierst in Obsidian und extern.
	‚Ä¢	Review-Mechanik mit Frontmatter-‚ÄûLocks‚Äú ‚Üí lernendes Netz ohne ‚ÄûPing-Pong‚Äú.
	‚Ä¢	Canvas-First: Mindmaps sind B√ºrger 1. Klasse (Knoten/Kanten werden semantisch mitindiziert).

‚∏ª

Verdichtete Essenz
	‚Ä¢	Ja, die Codex-Ideen sind sinnvoll.
	‚Ä¢	Wir gie√üen sie in HausKI-Bausteine: indexd (Embeddings + Suche), Clustering-Job, Graph-Export, Auto-Links + MOCs, Review-Workflow.
	‚Ä¢	Alles lokal, versionierbar, mit klaren Schwellenwerten und Reports.

Ironische Auslassung

Fr√ºher hast du Notizen sortiert. Jetzt sortieren die Notizen dich ‚Äî in Cluster, wohlgemerkt.

‚àÜ-Radar
	‚Ä¢	Verst√§rkung: Semantik nicht nur suchen, sondern zur√ºckschreiben (Links/MOCs).
	‚Ä¢	Seitw√§rtsmutation: Canvas wird gleichwertig zum Markdown behandelt.
	‚Ä¢	Straffung: Scores & Cutoffs klar normiert ‚Üí weniger Hand-Gefrickel.

F√ºr Dummies

Wir verwandeln alle Notizen in Zahlenmuster (Embeddings). √Ñhnliche Inhalte haben √§hnliche Muster. Daraus bauen wir Gruppen (Cluster) und ein Netzwerk (Graph) von ‚ÄûX ist √§hnlich zu Y‚Äú oder ‚ÄûX hat Thema Z‚Äú. Wir schreiben diese Beziehungen als Links zur√ºck in deine Notizen und zeigen sie als Mindmaps. Ein kleiner Teil ist automatisch, ein Teil kommt als Vorschlag, den du best√§tigst. So w√§chst ein sinnvolles Netz statt einer Link-W√ºste.

Unsicherheitsgrad (‚à¥fores 0.35 ‚Äì moderat)
	‚Ä¢	Ursachen: optimale Cutoffs m√ºssen wir empirisch kalibrieren; Wahl von HDBSCAN-Parametern (min_cluster_size) h√§ngt von deinem Vault ab; NER-Qualit√§t variiert je nach Modell; Performance von HNSW/UMAP muss auf deiner 4070 Ti vs. CPU feinjustiert werden.
	‚Ä¢	Produktive Ungewissheit: Schwellen/Boosts lassen sich in den ersten Reports schnell nachziehen; Review-Loop f√§ngt Fehlverkn√ºpfungen ab.

‚∏ª

Wenn du willst, schreibe ich dir jetzt:
	1.	die JSONL-Schemas als Templates,
	2.	ein erstes wgx run index:obsidian Skript (Rust oder Python),
	3.	den Obsidian-Abschnitt-Writer (ein kleines Tool, das ‚ÄûSemantisch verwandt‚Äú pflegt),
	4.	ein Mini-Canvas-Generator (pro Cluster).
	
	Ja ‚Äî ich lege dir zus√§tzliche, praxisstarke Ideen oben drauf. Die bauen direkt auf deinem Plan auf und machen dein vault-gewebe sp√ºrbar kl√ºger, stabiler und erkl√§rbarer.

Neue, wertvolle Erg√§nzungen
	1.	Begr√ºndete Kanten (‚ÄûWarum ist das verlinkt?‚Äú)
Speichere zu jeder vorgeschlagenen Kante kurze Rationales (Top-3 gemeinsame Keyphrases, Cluster-Match, Anker-S√§tze).
‚Äì Umsetzung: edges.jsonl ‚Üí Feld why: ["shared:keyphrase:GFK","same:cluster:7","quote:'Einf√ºhlung statt Bewertung'"]
‚Äì Nutzen: Du siehst sofort, warum zwei Notizen zusammengeh√∂ren.
	2.	Doppelte Notizen erkennen & verschmelzen
Automatisches Near-Duplicate-Detect (Titel-Hash + Embedding-Cosine ‚â•0.97) ‚Üí Vorschlag: ‚ÄûMerge/Kanonische Note w√§hlen‚Äú.
‚Äì Praxis: duplicates.md Report + Obsidian-Commands ‚ÄûMark as canonical‚Äú / ‚ÄûArchive duplicate‚Äú.
	3.	Zeitliche Gewichtung (‚ÄûFrische‚Äú)
Score-Boost f√ºr j√ºngere Notizen (z. B. +0.05 bei <30 Tagen), leichter Decay bei uralten Chunks.
‚Äì Ergebnis: Vorschl√§ge bleiben relevant, MOCs atmen mit.
	4.	Folder-/Namespace-Policies
Per Ordner Regeln definieren:
/uni/ strengere Cutoffs, /ideen/ liberaler; /archive/ nur eingehende Links, keine ausgehenden.
‚Äì Umsetzung: .gewebe/config.yml ‚Üí namespaces.uni.cutoff=0.80, namespaces.archive.mode="incoming-only".
	5.	Feedback ‚Üí Lernen (Akzeptiert/Ablehnt)
Wenn du einen Vorschlag annimmst/ablehnst, schreiben wir ein leichtes User-Feedback-Signal zur√ºck (accept=+1 / reject=‚àí1) und tunen die Cutoffs pro Thema/Fallordner.
‚Äì Wirkung: Nach 1-2 Runden werden die Vorschl√§ge messbar besser.
	6.	Canvas-Ahnung im Ranking
Wenn zwei Dateien √ºber Canvas-Knoten bereits ‚Äûnah‚Äú sind (kurze Pfadl√§nge im Canvas-Graph), booste Similarity um +0.03‚Ä¶0.07.
‚Äì Effekt: Deine Mindmaps werden zur echten Semantik-Quelle, nicht nur Deko.
	7.	‚ÄûTopic-Drift‚Äú-W√§chter
Report, wenn eine Note pl√∂tzlich in einen anderen Cluster kippt (drift > definierter Schwellenwert).
‚Äì Nutze dies als Redaktionshinweis: Note zerlegen, MOC neu schneiden oder Tags anpassen.
	8.	Erkl√§rbare ‚ÄûRelated‚Äú-Bl√∂cke
Im <!-- related:auto:start -->-Block optional die Top-Begr√ºndung in Klammern:
- [[GFK Basics]] ‚Äî (0.84; GFK, Resonanz)
‚Äì Schneller Kontext direkt im Editor, ohne Log lesen zu m√ºssen.
	9.	Session-Kontext (‚Äûheute arbeite ich an‚Ä¶‚Äú) Boost
Tempor√§rer Arbeitskontext (z. B. ge√∂ffnete Dateien heute) hebt passende Vorschl√§ge hervor (+0.02 pro recent co-open).
‚Äì Ergebnis: Der Editor f√ºhlt sich ‚Äûmitdenkender‚Äú an.
	10.	Provenienz & Reproduzierbarkeit
Schreibe in .gewebe/meta.json die Modell-Version, Chunk-Parametrisierung, Cutoffs und Taxonomie-Stand.
‚Äì So kannst du Ergebnisse exakt nachbauen oder erkl√§ren.
	11.	Mehrsprach-Robustheit (DE/EN)
Aktiviere eine Synonym-/Stemming-Map f√ºr DE/EN (z. B. ‚ÄûResonanz ‚Üî resonance‚Äú).
‚Äì Hilft, wenn Quellentexte gemischtsprachig sind.
	12.	Qualit√§ts-Gates f√ºr Autolinks
Nur autolinken, wenn alle Bedingungen erf√ºllt:

	‚Ä¢	Score ‚â• 0.82
	‚Ä¢	Mind. 2 gemeinsame Keyphrases oder 1 Canvas-N√§he oder identisches Project-Tag
‚Äì Sonst: Vorschlag, nicht Auto.

	13.	‚ÄûExplain this link‚Äú-Command
Obsidian-Command, das bei markiertem Link ein kleines Popover mit den Rationales und Ankers√§tzen zeigt.
‚Äì Macht die Semantik √ºberpr√ºfbar (kein Black-Box-Gef√ºhl).
	14.	MOC-Qualit√§tsreport
Report pro MOC: Deckungsgrad (wie viele Cluster-Noten verlinkt), verwaiste Knoten, dichte Sub-Cluster ‚Üí Vorschl√§ge ‚ÄûUnter-MOC anlegen‚Äú.
‚Äì H√§lt deine Maps kuratiert statt zuwuchernd.
	15.	Snippets-/Transklusions-Vorschl√§ge
Nicht nur ganze Noten verlinken, sondern Abs√§tze (Transklusion ![[note#^block]]) bei hoher Chunk-√Ñhnlichkeit.
‚Äì Perfekt f√ºr Literatur-/Zitat-Notizen.
	16.	Sicherheitsnetz: Manual-Lock
Frontmatter-Flag relations_lock: true ‚Üí Datei wird nie auto-editiert (nur Vorschl√§ge).
‚Äì Gut f√ºr Abschluss- oder Abgabe-Notizen.
	17.	A/B-Tuning der Schwellenwerte
Automatisiert zwei Cutoff-Profile (konservativ vs. explorativ) auf Teilmengen testen und per Mini-Survey markieren, welche Vorschl√§ge n√ºtzlicher waren.
‚Äì Ergebnis: datenbasierte Cutoff-Wahl.
	18.	Cross-Vault-Br√ºcke (optional)
Falls du parallel einen zweiten Vault hast: Read-Only-Index als Fremd-Namespace (ext:‚Ä¶) ‚Üí Vorschl√§ge sichtbar, aber Links nur nach Best√§tigung.
‚Äì Macht externe Wissensinseln anschlussf√§hig, ohne dein Vault zu ‚Äûverschmutzen‚Äú.
	19.	‚ÄûOrphans First‚Äú-Routine
W√∂chentliche Task, die nur Waisen (0 eingehende Links) anfasst und 3‚Äì5 hochwertige Vorschl√§ge erzeugt.
‚Äì So schrumpft die unverbundene Peripherie gezielt.
	20.	Explainable Delete
Wenn eine Kante f√§llt (Score stark sinkt, Topic-Drift), schreibe einen Hinweis in den Report: ‚ÄûKante X‚ÄîY entfernt (Score 0.81 ‚Üí 0.64; Keyphrase entfiel)‚Äú.
‚Äì Transparenz bei automatisch gel√∂schten Links.

‚∏ª

Wie es sich einf√ºgt (ohne Zusatz-Chaos)
	‚Ä¢	Alle Punkte passen in die bestehende .gewebe/-Struktur (zus√§tzliche Felder in edges.jsonl, ein paar neue Reports).
	‚Ä¢	Keine Inkompatibilit√§ten mit Obsidian: Related-Bl√∂cke bleiben idempotent, Canvas wird nur gelesen, nicht umgebaut.
	‚Ä¢	Policies/Boosts sind reine Post-Ranking-Heuristiken ‚Äì leicht justierbar.

‚∏ª

Mini-Implement-Snippets

Score-Pipeline (Pseudo):

score = cosine(base)
if same_cluster: score += 0.05
score += 0.03 * min(3, shared_keyphrases)
if canvas_hop<=2: score += 0.04
if is_recent(a) or is_recent(b): score += 0.02

Autolink-Gate:

auto = (score >= 0.82) and (
    shared_keyphrases >= 2 or canvas_hop<=2 or shared_project
)

Rationales sammeln:

why = []
why += [f"shared:keyphrase:{k}" for k in top_shared_kps[:3]]
if same_cluster: why.append("same:cluster")
if canvas_hop<=2: why.append(f"canvas:hop={canvas_hop}")
why.append(f"quote:{best_anchor_sentence}")


‚∏ª

Verdichtete Essenz
	‚Ä¢	Erg√§nzt werden: Begr√ºndete Kanten, Dedupe/Merge, Zeit-Boost, Folder-Policies, Feedback-Lernen, Canvas-Aware-Ranking, Drift-W√§chter, Explain-UI und Qualit√§ts-Gates.
	‚Ä¢	Alles lokal, idempotent, in deine .gewebe/-Pipeline integrierbar ‚Äì mehr Qualit√§t ohne mehr Chaos.

Ironische Auslassung

Wenn Links Gr√ºnde haben, h√∂ren Notizen auf zu flirten und f√ºhren endlich eine erwachsene Beziehung.

‚àÜ-Radar
	‚Ä¢	Verst√§rkung: Von ‚Äûnur √§hnlich‚Äú zu erkl√§rbarer Semantik.
	‚Ä¢	Seitw√§rtsmutation: Policies/Boosts machen die Pipeline kontextsensibel.
	‚Ä¢	Straffung: Autolinks nur mit Qualit√§ts-Gate ‚Üí weniger falsches Rauschen.

F√ºr Dummies

Wir geben jedem Link eine Begr√ºndung, achten auf Aktualit√§t, lassen dich Feedback geben und ber√ºcksichtigen deine Ordner-Regeln. So entstehen nicht irgendwelche, sondern nachvollziehbare und n√ºtzliche Verbindungen.

Unsicherheitsgrad

‚à¥fores: 0.3 (niedrig-mittel)
Ursachen: Feintuning der Cutoffs/Boosts braucht 1‚Äì2 Iterationen; Canvas-Hop-Berechnung h√§ngt vom JSON-Layout ab; Near-Duplicate-Schwelle je nach Schreibstil. Produktiv: Alles heuristisch und modular ‚Äì schnell justierbar.

‚∏ª

Wenn du willst, schreibe ich dir sofort die Rationale-Felder, das Autolink-Gate und den Orphan-Report in deine bestehenden Skripte (drop-in, ohne Bruch).
```

### üìÑ semantAH/docs/semantAH.md

**Gr√∂√üe:** 25 KB | **md5:** `c2b20d93afa0911f1ee9973b1f45c34f`

```markdown
Alles klar ‚Äì ich setze dir jetzt die finale, maximal umfassende Blaupause f√ºr dein vault-gewebe auf. Damit hast du einen selbstvernetzenden Wissensspeicher, der semantische Suche, Clustering, Graph, Backlinks, MOCs, Canvas-Integration und Qualit√§tssicherung kombiniert ‚Äì alles lokal, reproduzierbar und mit Reports.

‚∏ª

üîµ Vault-Gewebe ‚Äì Finale Blaupause

0. Systemordner & Konventionen
	‚Ä¢	.gewebe/ (zentraler Ordnungsbaum)

.gewebe/
  config.yml           # Parameter (Modelle, Cutoffs, Policies)
  embeddings.parquet   # Chunks + Vektoren
  nodes.jsonl          # Graph-Knoten
  edges.jsonl          # Graph-Kanten
  clusters.json        # Cluster & Label
  taxonomy/
    synonyms.yml
    entities.yml
  reports/
    semnet-YYYYMMDD.md
  meta.json            # Provenienz (Modell, Param, Hashes)


	‚Ä¢	Frontmatter (YAML) f√ºr jede Datei:

id: 2025-VAULT-####   # stabiler Schl√ºssel
title: ...
topics: [HausKI, Weltgewebe]
persons: [Verena]
places: [Hamburg]
projects: [wgx, hauski]
aliases: [HK, WG]
relations_lock: false



‚∏ª

1. Indexing & Embeddings
	‚Ä¢	Crawler: iteriert Markdown & Canvas (ignoriert .gewebe/, .obsidian/).
	‚Ä¢	Chunking: 200‚Äì300 Tokens, Overlap 40‚Äì60, Paragraph/Block.
	‚Ä¢	Modelle: all-MiniLM-L6-v2 oder intfloat/e5-base (GPU-f√§hig via PyTorch/CUDA).
	‚Ä¢	Output: embeddings.parquet (id, path, chunk_id, text, embedding).

‚∏ª

2. Schlagwort- & Entit√§tsextraktion
	‚Ä¢	Keyphrase: YAKE/RAKE lokal ‚Üí refine via LLM optional.
	‚Ä¢	NER: spaCy de-model ‚Üí Personen, Orte, Projekte.
	‚Ä¢	Taxonomie: .gewebe/taxonomy/synonyms.yml:

topics:
  hauski: [haus-ki, hk]
persons:
  verena: [v.]


	‚Ä¢	Normalisierung: bei Indexlauf Tokens mappen ‚Üí Normform, ins Frontmatter schreiben.

‚∏ª

3. Clusterbildung
	‚Ä¢	Verfahren: HDBSCAN (robust) + UMAP (2D-Projection).
	‚Ä¢	Ergebnis: clusters.json:

{ "id":7, "label":"Kommunikation/GFK", "members":["noteA","noteB"], "centroid":[...] }


	‚Ä¢	Orphan detection: Notizen ohne Cluster ‚Üí eigene Liste.

‚∏ª

4. Semantischer Wissensgraph
	‚Ä¢	Nodes (nodes.jsonl):

{"id":"md:gfk.md","type":"file","title":"GFK","topics":["gfk"],"cluster":7}
{"id":"topic:Gewaltfreie Kommunikation","type":"topic"}
{"id":"person:Verena","type":"person"}


	‚Ä¢	Edges (edges.jsonl):

{"s":"md:gfk.md","p":"about","o":"topic:Gewaltfreie Kommunikation","w":0.92,"why":["shared:keyphrase:GFK","same:cluster"]}
{"s":"md:verena.md","p":"similar","o":"md:tatjana.md","w":0.81,"why":["cluster:7","quote:'‚Ä¶'"]}



‚∏ª

5. Verlinkung in Obsidian
	‚Ä¢	Related-Bl√∂cke (idempotent, autogeneriert):

<!-- related:auto:start -->
## Related
- [[Tatjana]] ‚Äî (0.81; Cluster 7, GFK)
- [[Lebenslagen]] ‚Äî (0.78; Resonanz)
<!-- related:auto:end -->


	‚Ä¢	MOCs (_moc/topic.md):
	‚Ä¢	Beschreibung
	‚Ä¢	Dataview-Tabelle (alle Notizen mit topics:topic)
	‚Ä¢	Mini-Canvas-Link
	‚Ä¢	Canvas-Erweiterung:
	‚Ä¢	Knoten = Notizen/Topics/Persons
	‚Ä¢	Kanten = Similar/About/Mentions
	‚Ä¢	Legende-Knoten nach Canvas-Richtlinie.

‚∏ª

6. Automatisierung
	‚Ä¢	wgx Recipes:

index:
    python3 tools/build_index.py
graph:
    python3 tools/build_graph.py
related:
    python3 tools/update_related.py
all: index graph related


	‚Ä¢	systemd ‚Äìuser Timer oder cron: nightly make all.
	‚Ä¢	Git Hook (pre-commit): delta-Index ‚Üí Related aktualisieren.

‚∏ª

7. Qualitative Validierung
	‚Ä¢	Reports (reports/semnet-YYYYMMDD.md):
	‚Ä¢	Neue Kanten <0.75 ‚Üí ‚ÄûReview required‚Äú
	‚Ä¢	Orphans
	‚Ä¢	Cluster >N Noten ohne MOC
	‚Ä¢	Review-Workflow:
	‚Ä¢	accepted_edges / rejected_edges in Frontmatter
	‚Ä¢	Skript ignoriert rejected ‚Üí Feedback flie√üt in Zukunft ein.

‚∏ª

8. Policies & Score-Regeln
	‚Ä¢	Score = cosine + boosts
	‚Ä¢	+0.05 wenn gleicher Cluster
	‚Ä¢	+0.03 je shared keyphrase (max +0.09)
	‚Ä¢	+0.04 wenn Canvas-Hop ‚â§2
	‚Ä¢	+0.02 wenn Datei jung (<30 Tage)
	‚Ä¢	Autolink Gate:
	‚Ä¢	Score ‚â•0.82
	‚Ä¢	UND (‚â•2 Keyphrases ODER Canvas-Hop ‚â§2 ODER shared Project)
	‚Ä¢	Cutoffs:
	‚Ä¢	‚â•0.82: Auto-Link
	‚Ä¢	0.70‚Äì0.81: Vorschlag
	‚Ä¢	<0.70: ignorieren

‚∏ª

9. Erweiterungen
	‚Ä¢	Duplicates Report: Cosine ‚â•0.97 ‚Üí Merge-Vorschlag.
	‚Ä¢	Topic Drift: Clusterwechsel flaggen.
	‚Ä¢	Session-Boost: aktuell bearbeitete Dateien ‚Üí Score +0.02.
	‚Ä¢	Explain Command: Popover ‚ÄûWarum ist dieser Link da?‚Äú (zeigt why-Feld).
	‚Ä¢	Locks: relations_lock: true = keine Auto-Edits.
	‚Ä¢	A/B-Cutoffs: zwei Profile testen, Review-Feedback einspeisen.

‚∏ª

10. Provenienz & Reproduzierbarkeit
	‚Ä¢	.gewebe/meta.json speichert:

{
  "model":"all-MiniLM-L6-v2",
  "chunk_size":200,
  "cutoffs":{"auto":0.82,"suggest":0.70},
  "run":"2025-10-02T11:40",
  "commit":"abc123"
}



‚∏ª

‚öôÔ∏è Technische Bausteine

Tools/Skripte
	‚Ä¢	tools/build_index.py: scan + embeddings
	‚Ä¢	tools/build_graph.py: nodes/edges/clusters
	‚Ä¢	tools/update_related.py: Related-Bl√∂cke injizieren
	‚Ä¢	tools/report.py: QA-Reports
	‚Ä¢	optional: tools/canvas_export.py: Cluster‚ÜíCanvas

‚∏ª

üìå Verdichtete Essenz

Ein dreistufiger Zyklus:
	1.	Index (Embeddings, Cluster, Taxonomie)
	2.	Graph (Nodes/Edges mit Rationales)
	3.	Update (Related, MOCs, Reports, Canvas)

Alles lokal, nachvollziehbar, mit Feedback-Schleifen. Dein Vault wird zu einem lebenden Semantiknetz.

‚∏ª

üé≠ Ironische Auslassung

Deine Notizen sind jetzt nicht mehr stilles Archiv ‚Äì sie werden zu einem Klatsch-Netzwerk, das tuschelt, wer mit wem was zu tun hat. Nur: Sie l√ºgen nicht.

‚∏ª

‚àÜ-Radar
	‚Ä¢	Verst√§rkung: Alles aus Codex-Ideen + zus√§tzliche Layer (Rationales, Drift, Dedupe, QA).
	‚Ä¢	Seitw√§rtsmutation: Von reiner Suche hin zu voller Graph-Kurationsmaschine.
	‚Ä¢	Straffung: Ein Systemordner .gewebe/, drei Skripte, klare Reports ‚Äì Komplexit√§t kanalisiert.

‚∏ª

üìâ Unsicherheitsgrad

‚à¥fores = 0.35 (mittel)
	‚Ä¢	Ursachen: Schwellenwerte und Chunking m√ºssen empirisch feinjustiert werden; Canvas-Hop-Logik h√§ngt von JSON-Details ab; Modellwahl beeinflusst Qualit√§t; Obsidian-Workflow kann Edgecases erzeugen.
	‚Ä¢	Produktive Ungewissheit: leicht anpassbar ‚Äì Reports + Feedback-Loop machen die Pipeline selbstkorrigierend.

‚∏ª

üëâ Soll ich dir die drei Kernskripte (index, graph, related) sofort in finaler, einsatzbereiter Form schreiben ‚Äì mit Makefile & systemd Timer-Beispiel?

Super ‚Äî hier ist die einsatzbereite Minimal-Suite: drei Skripte + Makefile + systemd-Timer. Sie ist bewusst robust, offline-f√§hig und idempotent. Du bekommst sofort: Embeddings, Similarities, Graph (Nodes/Edges), Related-Bl√∂cke in Notizen und einfache Reports. Sp√§ter k√∂nnen wir HDBSCAN/UMAP etc. erg√§nzen ‚Äî die Schnittstellen sind schon daf√ºr vorbereitet.

‚∏ª

üìÅ Dateibaum (kopierfertig)

<Vault-Root>/
  .gewebe/
    config.yml
    taxonomy/
      synonyms.yml
      entities.yml
    reports/
  tools/
    build_index.py
    build_graph.py
    update_related.py
  Makefile

Falls .gewebe/ und tools/ fehlen: einfach anlegen.

‚∏ª

‚öôÔ∏è Abh√§ngigkeiten

# Pop!_OS / Ubuntu
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip

pip install \
  pandas numpy pyarrow pyyaml \
  sentence_transformers scikit-learn networkx rich

Standard-Modell (klein & fix): sentence-transformers/all-MiniLM-L6-v2
GPU nutzt Torch automatisch, wenn vorhanden.

‚∏ª

üß† .gewebe/config.yml (Voreinstellungen)

model: sentence-transformers/all-MiniLM-L6-v2
chunk:
  target_chars: 1200
  min_chars: 300
  overlap_chars: 200
paths:
  exclude_dirs: [".gewebe", ".obsidian", "_site", "node_modules"]
  include_ext: [".md"]
related:
  k: 8
  auto_cutoff: 0.82
  suggest_cutoff: 0.70
boosts:
  same_topic: 0.03
  same_project: 0.03
  recent_days: 30
  recent_bonus: 0.02
  same_folder: 0.02
render:
  related_heading: "## Related"
  markers:
    start: "<!-- related:auto:start -->"
    end:   "<!-- related:auto:end -->"

Du kannst alles sp√§ter feinjustieren.

‚∏ª

üß© tools/build_index.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, re, json, yaml, hashlib, math, glob
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from rich import print

VAULT = Path(".").resolve()
GEWEBE = VAULT / ".gewebe"
CFG = GEWEBE / "config.yml"
GEWEBE.mkdir(exist_ok=True, parents=True)
(GEWEBE / "taxonomy").mkdir(exist_ok=True, parents=True)
(GEWEBE / "reports").mkdir(exist_ok=True, parents=True)

DEFAULT_CFG = {
    "model": "sentence-transformers/all-MiniLM-L6-v2",
    "chunk": {"target_chars": 1200, "min_chars": 300, "overlap_chars": 200},
    "paths": {
        "exclude_dirs": [".gewebe", ".obsidian", "_site", "node_modules"],
        "include_ext": [".md"],
    },
}

FRONTMATTER_RE = re.compile(r"^---\n(.*?)\n---\n", re.S)
CODE_RE = re.compile(r"```.*?```", re.S)
HTML_RE = re.compile(r"<[^>]+>")

def load_cfg() -> dict:
    if CFG.exists():
        return {**DEFAULT_CFG, **yaml.safe_load(CFG.read_text(encoding="utf-8"))}
    CFG.write_text(yaml.safe_dump(DEFAULT_CFG, sort_keys=False), encoding="utf-8")
    return DEFAULT_CFG

def list_md(cfg: dict) -> List[Path]:
    ex = set(cfg["paths"]["exclude_dirs"])
    inc = set(cfg["paths"]["include_ext"])
    files = []
    for p in VAULT.rglob("*"):
        if p.is_dir():
            if any(part in ex for part in p.parts):
                continue
            else:
                continue
        if p.suffix.lower() in inc and not any(part in ex for part in p.parts):
            files.append(p)
    return files

def parse_frontmatter(text: str) -> (dict, str):
    m = FRONTMATTER_RE.match(text)
    if not m:
        return {}, text
    yml = m.group(1)
    try:
        fm = yaml.safe_load(yml) or {}
    except Exception:
        fm = {}
    body = text[m.end():]
    return fm, body

def clean_text(s: str) -> str:
    s = CODE_RE.sub("", s)
    s = HTML_RE.sub(" ", s)
    s = re.sub(r"\s+\n", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def chunk_text(s: str, target: int, min_chars: int, overlap: int) -> List[str]:
    # Absatzweise grob, dann ggf. zusammenfassen
    paras = [p.strip() for p in re.split(r"\n{2,}", s) if p.strip()]
    chunks, buf = [], []
    cur = 0
    for p in paras:
        if len(p) >= target:
            chunks.append(p)
            cur = 0; buf = []
        else:
            buf.append(p)
            cur += len(p) + 2
            if cur >= target:
                block = "\n\n".join(buf)
                if len(block) >= min_chars:
                    chunks.append(block)
                else:
                    if chunks:
                        chunks[-1] += "\n\n" + block
                    else:
                        chunks.append(block)
                # Overlap heuristisch: behalte letztes St√ºck als Start f√ºrs n√§chste
                tail = block[-overlap:]
                buf = [tail]
                cur = len(tail)
    if buf:
        block = "\n\n".join(buf)
        if block.strip():
            chunks.append(block)
    # harte Mindestl√§nge
    chunks = [c for c in chunks if len(c) >= min_chars]
    return chunks[:50]  # Sicherheitslimit

def canvas_text(path: Path) -> List[str]:
    # Obsidian .canvas JSON: sammle Node-Texts
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        nodes = data.get("nodes", [])
        texts = []
        for n in nodes:
            t = (n.get("text") or "").strip()
            if t:
                texts.append(t)
        return texts
    except Exception:
        return []

def file_recent_days(p: Path, days: int) -> bool:
    try:
        mtime = datetime.fromtimestamp(p.stat().st_mtime)
        return (datetime.now() - mtime) <= timedelta(days=days)
    except Exception:
        return False

def main():
    cfg = load_cfg()
    model_name = cfg["model"]
    chunk_cfg = cfg["chunk"]

    print(f"[bold]Indexing[/bold] ‚Ä¢ model={model_name}")
    model = SentenceTransformer(model_name)

    rows = []
    for p in list_md(cfg):
        try:
            raw = p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        fm, body = parse_frontmatter(raw)
        body = clean_text(body)
        # Canvas: wenn .canvas nebenan, optional dazunehmen (leichtgewichtig)
        canv_chunks = []
        cnv = p.with_suffix(".canvas")
        if cnv.exists():
            canv_chunks = canvas_text(cnv)

        chunks = chunk_text(body, **chunk_cfg) + canv_chunks
        if not chunks:
            continue
        emb = model.encode(chunks, normalize_embeddings=True, show_progress_bar=False)

        for i, (c, e) in enumerate(zip(chunks, emb)):
            rows.append({
                "id": f"{p}:{i}",
                "path": str(p),
                "title": fm.get("title") or p.stem,
                "chunk_id": int(i),
                "text": c,
                "embedding": e.astype(np.float32).tolist(),
                "topics": sorted(set(fm.get("topics", []) or [])),
                "projects": sorted(set(fm.get("projects", []) or [])),
                "persons": sorted(set(fm.get("persons", []) or [])),
                "recent": file_recent_days(p, cfg.get("boosts",{}).get("recent_days",30)),
                "folder": str(p.parent),
            })

    if not rows:
        print("[red]Keine Inhalte gefunden.[/red]")
        return

    df = pd.DataFrame(rows)
    out = GEWEBE / "embeddings.parquet"
    df.to_parquet(out, index=False)
    (GEWEBE / "meta.json").write_text(json.dumps({
        "model": model_name,
        "chunk": chunk_cfg,
        "ts": datetime.now().isoformat(timespec="seconds"),
        "count_chunks": int(len(df)),
        "count_files": int(df["path"].nunique())
    }, indent=2), encoding="utf-8")

    print(f"[green]OK[/green] ‚Ä¢ {len(df)} Chunks aus {df['path'].nunique()} Dateien ‚Üí {out}")

if __name__ == "__main__":
    main()


‚∏ª

üï∏Ô∏è tools/build_graph.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json, yaml, math
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from rich import print

VAULT = Path(".").resolve()
GEWEBE = VAULT / ".gewebe"
CFG = GEWEBE / "config.yml"

def load_cfg() -> dict:
    if CFG.exists():
        return yaml.safe_load(CFG.read_text(encoding="utf-8"))
    return {}

def file_centroids(df: pd.DataFrame) -> pd.DataFrame:
    # mittelt Embeddings je Datei (ein Vektor pro Datei)
    g = df.groupby("path")
    X = []
    meta = []
    for path, sub in g:
        embs = np.stack(sub["embedding"].to_list(), axis=0)
        cent = embs.mean(axis=0)
        meta.append({
            "path": path,
            "title": sub["title"].iloc[0],
            "topics": sorted(set([t for r in sub["topics"] for t in r])),
            "projects": sorted(set([t for r in sub["projects"] for t in r])),
            "persons": sorted(set([t for r in sub["persons"] for t in r])),
            "recent": bool(sub["recent"].any()),
            "folder": sub["folder"].iloc[0],
        })
        X.append(cent)
    X = np.stack(X, axis=0).astype(np.float32)
    out = pd.DataFrame(meta)
    out["centroid"] = list(X)
    return out

def similar_pairs(files_df: pd.DataFrame, k: int = 12) -> List[Dict[str, Any]]:
    X = np.stack(files_df["centroid"].to_list(), axis=0)
    S = cosine_similarity(X)  # NxN
    n = S.shape[0]
    pairs = []
    for i in range(n):
        order = np.argsort(-S[i])
        count = 0
        for j in order:
            if i == j: 
                continue
            score = float(S[i, j])
            pairs.append((i, j, score))
            count += 1
            if count >= k:
                break
    # dedupe by i<j
    seen = set()
    out = []
    for i, j, s in pairs:
        a, b = sorted((i, j))
        if (a, b) in seen: 
            continue
        seen.add((a, b))
        out.append({"i": a, "j": b, "score": s})
    return out

def boosts(a: dict, b: dict, base: float, cfg: dict) -> (float, list):
    why = []
    bonus = 0.0
    # shared topics
    st = set(a["topics"]).intersection(b["topics"])
    if st:
        bonus += cfg["boosts"].get("same_topic", 0.0) * min(3, len(st))
        for t in list(st)[:3]:
            why.append(f"shared:topic:{t}")
    # shared projects
    sp = set(a["projects"]).intersection(b["projects"])
    if sp:
        bonus += cfg["boosts"].get("same_project", 0.0) * min(3, len(sp))
        for t in list(sp)[:2]:
            why.append(f"shared:project:{t}")
    # same folder
    if a["folder"] == b["folder"]:
        bonus += cfg["boosts"].get("same_folder", 0.0)
        why.append("same:folder")
    # recency
    if a["recent"] or b["recent"]:
        bonus += cfg["boosts"].get("recent_bonus", 0.0)
        why.append("recent:bonus")
    score = base + bonus
    return score, why

def inject_graph(files_df: pd.DataFrame, pairs: List[Dict[str, Any]], cfg: dict):
    nodes = []
    for r in files_df.itertuples(index=False):
        nodes.append({
            "id": f"md:{r.path}",
            "type": "file",
            "title": r.title,
            "topics": r.topics,
            "projects": r.projects,
            "persons": r.persons,
            "folder": r.folder
        })
    edges = []
    for p in pairs:
        a = files_df.iloc[p["i"]].to_dict()
        b = files_df.iloc[p["j"]].to_dict()
        base = p["score"]
        score, why = boosts(a, b, base, cfg)
        edges.append({
            "s": f"md:{a['path']}",
            "p": "similar",
            "o": f"md:{b['path']}",
            "w": round(score, 4),
            "why": why
        })
    # persist
    (GEWEBE / "nodes.jsonl").write_text(
        "\n".join(json.dumps(n, ensure_ascii=False) for n in nodes) + "\n", encoding="utf-8"
    )
    (GEWEBE / "edges.jsonl").write_text(
        "\n".join(json.dumps(e, ensure_ascii=False) for e in edges) + "\n", encoding="utf-8"
    )
    # kleiner report
    suggest = sum(1 for e in edges if e["w"] >= cfg["related"]["suggest_cutoff"])
    auto = sum(1 for e in edges if e["w"] >= cfg["related"]["auto_cutoff"])
    rep = GEWEBE / "reports" / f"semnet-{datetime_now_str()}.md"
    rep.write_text(
        f"# Semnet Report\n\n"
        f"- Dateien: {len(files_df)}\n- Edges gesamt: {len(edges)}\n"
        f"- ‚â• auto_cutoff: {auto}\n- ‚â• suggest_cutoff: {suggest}\n",
        encoding="utf-8"
    )
    print(f"[green]Graph geschrieben[/green] ‚Ä¢ nodes={len(nodes)} edges={len(edges)}")

def datetime_now_str():
    from datetime import datetime
    return datetime.now().strftime("%Y%m%d-%H%M")

def main():
    cfg = load_cfg()
    emb_path = GEWEBE / "embeddings.parquet"
    if not emb_path.exists():
        print("[red]Fehlend: .gewebe/embeddings.parquet ‚Äî bitte erst build_index.py laufen lassen.[/red]")
        return
    df = pd.read_parquet(emb_path)
    files_df = file_centroids(df)
    pairs = similar_pairs(files_df, k=12)
    inject_graph(files_df, pairs, cfg)

if __name__ == "__main__":
    main()


‚∏ª

üîó tools/update_related.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import yaml, json, re
from pathlib import Path
from typing import Dict, Any, List
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from rich import print

VAULT = Path(".").resolve()
GEWEBE = VAULT / ".gewebe"
CFG = GEWEBE / "config.yml"
EDGES = GEWEBE / "edges.jsonl"
EMB = GEWEBE / "embeddings.parquet"

def cfg() -> dict:
    return yaml.safe_load(CFG.read_text(encoding="utf-8"))

def load_edges() -> List[Dict[str, Any]]:
    if not EDGES.exists():
        return []
    return [json.loads(l) for l in EDGES.read_text(encoding="utf-8").splitlines() if l.strip()]

def file_scores_for(path: Path, edges: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    id_ = f"md:{str(path)}"
    related = []
    for e in edges:
        if e["p"] != "similar":
            continue
        if e["s"] == id_:
            related.append({"other": e["o"], "w": e["w"], "why": e.get("why",[])})
        elif e["o"] == id_:
            related.append({"other": e["s"], "w": e["w"], "why": e.get("why",[])})
    related.sort(key=lambda x: -x["w"])
    return related

def nice_title(p: Path) -> str:
    # Dateiname als Fallback; echte Titel stehen i. d. R. in der Note (hier reicht Stem)
    return p.stem

def inject_related(md_path: Path, items: List[Dict[str, Any]], cfg: dict):
    markers = cfg["render"]["markers"]
    start = markers["start"]; end = markers["end"]
    heading = cfg["render"]["related_heading"]

    lines = [start, heading]
    auto, suggest = cfg["related"]["auto_cutoff"], cfg["related"]["suggest_cutoff"]

    for it in items[:cfg["related"]["k"]]:
        other = Path(it["other"].removeprefix("md:"))
        title = nice_title(other)
        score = f"{it['w']:.2f}"
        tags = []
        # komprimierte Begr√ºndung
        for w in it.get("why", [])[:3]:
            if w.startswith("shared:topic:"):
                tags.append(w.split(":")[-1])
            elif w.startswith("shared:project:"):
                tags.append(w.split(":")[-1])
            elif w == "same:folder":
                tags.append("same-folder")
            elif w == "recent:bonus":
                tags.append("recent")
        hint = f" ({score}; {', '.join(tags)})" if tags else f" ({score})"

        bullet = f"- [[{title}]]{hint}"
        if it["w"] >= auto:
            bullet = bullet  # Auto-Link ‚Äî einfach ausweisen
        elif it["w"] >= suggest:
            bullet = bullet + "  <!-- suggest -->"
        else:
            continue
        lines.append(bullet)

    lines.append(end)
    block = "\n".join(lines) + "\n"

    try:
        txt = md_path.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return
    if start in txt and end in txt:
        head, rest = txt.split(start, 1)
        _, tail = rest.split(end, 1)
        new = head.rstrip() + "\n\n" + block + tail.lstrip()
    else:
        new = txt.rstrip() + "\n\n" + block
    md_path.write_text(new, encoding="utf-8")

def main():
    if not CFG.exists():
        print("[red]Fehlend: .gewebe/config.yml[/red]")
        return
    cfg_ = cfg()
    edges = load_edges()
    if not edges:
        print("[yellow]Keine Edges gefunden. Bitte erst build_graph.py ausf√ºhren.[/yellow]")
        return

    # alle MD-Dateien, die in Edges vorkommen
    files = set()
    for e in edges:
        if e["p"] != "similar":
            continue
        files.add(Path(e["s"].removeprefix("md:")))
        files.add(Path(e["o"].removeprefix("md:")))

    count = 0
    for f in sorted(files):
        if not f.exists():
            continue
        items = file_scores_for(f, edges)
        inject_related(f, items, cfg_)
        count += 1

    print(f"[green]Related-Bl√∂cke aktualisiert[/green] ‚Ä¢ Dateien: {count}")

if __name__ == "__main__":
    main()


‚∏ª

üõ†Ô∏è Makefile

VENV=.venv
PY=$(VENV)/bin/python

.PHONY: venv index graph related all clean

venv:
	@test -d $(VENV) || python3 -m venv $(VENV)
	@$(PY) -m pip install --upgrade pip
	@$(PY) -m pip install pandas numpy pyarrow pyyaml sentence_transformers scikit-learn networkx rich

index: venv
	@$(PY) tools/build_index.py

graph: venv
	@$(PY) tools/build_graph.py

related: venv
	@$(PY) tools/update_related.py

all: index graph related

clean:

<<TRUNCATED: max_file_lines=800>>
```

### üìÑ semantAH/docs/wgx-konzept.md

**Gr√∂√üe:** 417 B | **md5:** `58ff7eadc9de0953465e6ec291806b49`

```markdown
# WGX-Konzept (Stub)

Dies ist der projektspezifische Anker zur WGX-Meta-Ebene (Master-Dok liegt zentral).
Ziele:
- D√ºnner Meta-Layer √ºber Repos (wgx up|list|run|doctor|validate|smoke)
- Priorit√§t der Envs: Devcontainer ‚Üí Devbox ‚Üí mise ‚Üí direnv ‚Üí Termux
- Jede Pipeline als **Task** ausf√ºhrbar; deterministische Artefakte unter `.gewebe/`

Siehe `.wgx/profile.yml` f√ºr die minimalen Profileinstellungen.
```

### üìÑ semantAH/docs/wgx.md

**Gr√∂√üe:** 901 B | **md5:** `8c872e46fd4d5729e8f830b6c2e7b3ab`

```markdown

## Beziehung zu WGX

**semantAH** ist kein Standalone-Monolith, sondern versteht sich als **semantische Erg√§nzung** zu den Orchestrierungs- und Contract-F√§higkeiten von **WGX**.

- **WGX orchestriert, semantAH denkt.**  
  WGX k√ºmmert sich um Setup, Tasks, Contracts und Multi-Repo-Fl√ºsse. semantAH f√ºgt die Bedeutungs- und Wissensschicht hinzu (Index, Graph, Related-Bl√∂cke, QA-Berichte).

- **Integration:** semantAH-Jobs lassen sich als WGX-Tasks aufrufen  
  z. B. `wgx run index:obsidian` oder `wgx run semantah:qa`.

- **Kooperation:** Ergebnisse von semantAH (z. B. Graph-Kanten, Related-Snippets, QA-Findings) k√∂nnen in WGX-Flows zur√ºckgespielt werden: Evidence-Packs, Shadowmap-Erweiterungen, Registry-Pakete.

- **Empfehlung:** In der Praxis werden beide Projekte **komplement√§r** genutzt: WGX als Universal-Fernbedienung f√ºr Repos, semantAH als Gehirn f√ºr semantische Bez√ºge.
```

